{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm \n",
    "from tensorflow.keras import *\n",
    "\n",
    "\n",
    "train_token_path = \"./data/imdb/train_token.csv\"\n",
    "test_token_path = \"./data/imdb/test_token.csv\"\n",
    "\n",
    "MAX_WORDS = 10000  # We will only consider the top 10,000 words in the dataset\n",
    "MAX_LEN = 200  # We will cut reviews after 200 words\n",
    "BATCH_SIZE = 20 \n",
    "\n",
    "# 构建管道\n",
    "def parse_line(line):\n",
    "    t = tf.strings.split(line,\"\\t\")\n",
    "    label = tf.reshape(tf.cast(tf.strings.to_number(t[0]),tf.int32),(-1,))\n",
    "    features = tf.cast(tf.strings.to_number(tf.strings.split(t[1],\" \")),tf.int32)\n",
    "    return (features,label)\n",
    "\n",
    "ds_train=  tf.data.TextLineDataset(filenames = [train_token_path]) \\\n",
    "   .map(parse_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test=  tf.data.TextLineDataset(filenames = [test_token_path]) \\\n",
    "   .map(parse_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一，Sequential按层顺序创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 7)            70000     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 196, 64)           2304      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 98, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 96, 32)            6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 48, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 1537      \n",
      "=================================================================\n",
      "Total params: 80,017\n",
      "Trainable params: 80,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
    "model.add(layers.MaxPool1D(2))\n",
    "model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
    "model.add(layers.MaxPool1D(2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1,activation = \"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer='Nadam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[\"AUC\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 17s 15ms/step - loss: 0.4885 - auc: 0.8286 - val_loss: 0.3200 - val_auc: 0.9387\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-50c2006dfebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data/keras_model/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%d-%H%M%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtensorboard_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistogram_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbaselogger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1228\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    918\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m           \u001b[0;31m# Make value available to next callbacks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'metrics'"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "baselogger = callbacks.BaseLogger(stateful_metrics=[\"AUC\"])\n",
    "logdir = \"./data/keras_model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "history = model.fit(ds_train,validation_data = ds_test,\n",
    "        epochs = 6,callbacks=[baselogger,tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(history, metric):\n",
    "    train_metrics = history.history[metric]\n",
    "    val_metrics = history.history['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'AUC'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0157a4bc95b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"AUC\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-a3af51accb9e>\u001b[0m in \u001b[0;36mplot_metric\u001b[0;34m(history, metric)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_metrics\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'AUC'"
     ]
    }
   ],
   "source": [
    "plot_metric(history,\"AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三，Model子类化创建自定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先自定义一个残差模块，为自定义Layer\n",
    "\n",
    "class ResBlock(layers.Layer):\n",
    "    def __init__(self, kernel_size, **kwargs):\n",
    "        super(ResBlock, self).__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "    \n",
    "    def build(self,input_shape):\n",
    "        self.conv1 = layers.Conv1D(filters=64,kernel_size=self.kernel_size,\n",
    "                                   activation = \"relu\",padding=\"same\")\n",
    "        self.conv2 = layers.Conv1D(filters=32,kernel_size=self.kernel_size,\n",
    "                                   activation = \"relu\",padding=\"same\")\n",
    "        self.conv3 = layers.Conv1D(filters=input_shape[-1],\n",
    "                                   kernel_size=self.kernel_size,activation = \"relu\",padding=\"same\")\n",
    "        self.maxpool = layers.MaxPool1D(2)\n",
    "        super(ResBlock,self).build(input_shape) # 相当于设置self.built = True\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = layers.Add()([inputs,x])\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "    \n",
    "    #如果要让自定义的Layer通过Functional API 组合成模型时可以序列化，需要自定义get_config方法。\n",
    "    def get_config(self):  \n",
    "        config = super(ResBlock, self).get_config()\n",
    "        config.update({'kernel_size': self.kernel_size})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 100, 7])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试ResBlock\n",
    "resblock = ResBlock(kernel_size = 3)\n",
    "resblock.build(input_shape = (None,200,7))\n",
    "resblock.compute_output_shape(input_shape=(None,200,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义模型，实际上也可以使用Sequential或者FunctionalAPI\n",
    "\n",
    "class ImdbModel(models.Model):\n",
    "    def __init__(self):\n",
    "        super(ImdbModel, self).__init__()\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.embedding = layers.Embedding(MAX_WORDS,7)\n",
    "        self.block1 = ResBlock(7)\n",
    "        self.block2 = ResBlock(5)\n",
    "        self.dense = layers.Dense(1,activation = \"sigmoid\")\n",
    "        super(ImdbModel,self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = self.dense(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"imdb_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  70000     \n",
      "_________________________________________________________________\n",
      "res_block (ResBlock)         multiple                  19143     \n",
      "_________________________________________________________________\n",
      "res_block_1 (ResBlock)       multiple                  13703     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  351       \n",
      "=================================================================\n",
      "Total params: 103,197\n",
      "Trainable params: 103,197\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = ImdbModel()\n",
    "model.build(input_shape =(None,200))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='Nadam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy',\"AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 36s 34ms/step - loss: 0.5396 - accuracy: 0.6867 - auc: 0.7823 - val_loss: 0.3433 - val_accuracy: 0.8522 - val_auc: 0.9286\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 32s 32ms/step - loss: 0.2614 - accuracy: 0.8932 - auc: 0.9586 - val_loss: 0.3111 - val_accuracy: 0.8676 - val_auc: 0.9428\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1486 - accuracy: 0.9443 - auc: 0.9862 - val_loss: 0.3904 - val_accuracy: 0.8622 - val_auc: 0.9378\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0751 - accuracy: 0.9750 - auc: 0.9958 - val_loss: 0.5322 - val_accuracy: 0.8592 - val_auc: 0.9294\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 30s 30ms/step - loss: 0.0350 - accuracy: 0.9880 - auc: 0.9989 - val_loss: 0.7348 - val_accuracy: 0.8570 - val_auc: 0.9151\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0171 - accuracy: 0.9945 - auc: 0.9996 - val_loss: 0.9910 - val_accuracy: 0.8512 - val_auc: 0.9028\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "logdir = \"./tflogs/keras_model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "history = model.fit(ds_train,validation_data = ds_test,\n",
    "                    epochs = 6,callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
